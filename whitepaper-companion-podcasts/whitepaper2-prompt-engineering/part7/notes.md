# Code Prompting

The whitepaper dedicates an entire section to code prompting, which is especially valuable for Kaggle users. Prompt engineering is not limited to natural language tasks. It is a powerful tool for generating, explaining, translating, and debugging code.

## Writing Code with Prompts

The paper begins with prompts designed to generate code, such as creating a Bash script. LLMs can greatly speed up development, but the paper emphasizes that generated code should always be reviewed and tested before use. Relying blindly on the model can lead to mistakes that disrupt your Kaggle environment.

### Deep dive into

* Creating safe prompts for complex code generation
* Reviewing model generated code effectively
* Recognizing common pitfalls in autogenerated scripts

---

## Explaining Code

The paper also covers prompting models to explain code. Using the earlier Bash script, the authors show how an LLM can describe what the script does in clear terms. This is useful for understanding unfamiliar code, learning new concepts, or collaborating on Kaggle teams.

### Deep dive into

* Best styles for code explanation prompts
* Teaching prompts for adding context or visuals
* Using explanations for code reviews in teams

---

## Translating Code Between Languages

Another practical use case is code translation. The paper demonstrates translating the Bash script into Python. This is helpful when you encounter an algorithm in a language you are less comfortable with. As always, double checking the translated result is essential.

### Deep dive into

* Language specific translation challenges
* Comparing translated code to original logic
* Testing translated code using Kaggle notebooks

---

## Debugging and Reviewing Code

Debugging is a universal challenge on Kaggle. The whitepaper includes an example where a broken Python script and its traceback are provided to the LLM. The model identifies the error and suggests a fix. In some cases, it even proposes improvements that make the code more robust and efficient.

It is like having a coding partner available at all times to assist with debugging, reviewing logic, and optimizing code.

### Deep dive into

* Prompt templates for error tracebacks
* Improving model suggested fixes
* Using debugging prompts systematically

---

# Multimodal Prompting

The paper briefly touches on multimodal prompting, which includes inputs beyond text, such as images or audio. This is still developing, but it may become important for Kaggle competitions that use multimodal datasets.

### Deep dive into

* Early techniques for multimodal LLM prompting
* Handling image plus text tasks
* Future Kaggle use cases for multimodal workflows

---

# Best Practices for Prompt Engineering

The final section outlines best practices that significantly elevate prompt performance, especially in Kaggle environments.

## Provide Examples

One shot and few shot prompting remain critical tools. Providing examples is one of the most effective ways to teach the model exactly what you want.

### Deep dive into

* Structuring examples for clarity
* Choosing examples that reinforce patterns
* Preventing confusion from inconsistent examples

---

## Keep Prompts Simple

Clarity is essential. Prompts should be short, direct, and easy to understand. Avoid jargon when possible, and use strong action verbs when requesting code or analysis.

---

## Be Specific About the Desired Output

If you need predictions in a particular CSV or JSON format, explicitly specify this. Leaving it vague can cause errors or formatting issues later.

---

## Use Instructions Instead of Constraints

The paper suggests focusing on positive instructions. Instead of saying “do not use this library,” say “write code that only uses these specific libraries.” Positive framing is often more effective.

---

## Control Max Token Length

To stay within Kaggle limits and keep processing efficient, always manage the maximum token output. This prevents overflow and excessive compute usage.

---

## Use Variables in Prompts

Variables help make prompts reusable and adaptable. You can adjust a variable for a new dataset or task without rewriting the entire prompt.

---

## Experiment with Input Formats and Wording

The paper encourages experimentation. Try phrasing prompts as questions, instructions, or statements. Different styles can lead to different levels of clarity and accuracy.

For classification tasks using few shot prompts, vary the order of classes within the examples to reduce bias.

---

## Stay Updated with New Models

The AI landscape changes quickly. New models and features appear frequently, and staying updated can give you an edge in competitions.

---

## Experiment with Output Formats

Structured formats such as JSON or CSV are often easier to use in Kaggle workflows. The paper even mentions JSON repair utilities that automatically fix formatting issues. Using JSON schemas can further reduce misinterpretations by defining clear expectations for the model.

---

## Collaborate with Other Prompt Engineers

Sharing prompt ideas and strategies can accelerate progress. Collaboration is an important part of Kaggle culture, and prompt engineering is no exception.

---

## Best Practices for Safe and Effective Reasoning

For tasks involving logical problem solving, such as those common on Kaggle, the paper suggests:

* Put the final answer after the reasoning steps
* Set temperature to zero for strict logical tasks

---

## Document All Prompt Attempts

Perhaps the most important best practice is thorough documentation. Keep detailed records of your prompt experiments, results, and feedback. This helps you track progress, diagnose issues, and refine your techniques over time. For platforms such as Vertex AI Studio, linking and saving prompts can simplify long term work.

If integrating prompts into your codebase, the paper provides guidance on versioning, testing, and maintaining prompts as part of your development workflow.

Prompt engineering is an iterative process. Improvement comes through experimentation, reflection, and refinement.

### Deep dive into

* Documentation templates for prompt logs
* Metrics for evaluating prompt quality
* Integrating prompt files into version control